# training loop
model = XceptionCustomBLITZ().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)
criterion = torch.nn.CrossEntropyLoss()  # BLiTZ averages inside sample_elbo

N = 10  
epochs = 3

for epoch in range(epochs):
    print(epoch)
    model.train()
    for xb, yb in train_sub_loader:
        xb, yb = xb.to(device), yb.to(device).long()
        optimizer.zero_grad()
        # BLiTZ: sample_elbo handles multiple forward passes and KL accumulation.
        # sample_nbr: number of MC weight samples to estimate expected likelihood (try 3-10)
        # complexity_cost_weight: typically 1/N (scale KL per datapoint).
        loss = model.sample_elbo(inputs=xb,
                                 labels=yb,
                                 criterion=criterion,
                                 sample_nbr=3) # number of MC samples
        print(loss)
        loss.backward()
        optimizer.step()

'''Note: sample_nbr: int -> The number of times of the weight-sampling and predictions done in our Monte-Carlo approach to gather the loss to be .backwarded in the optimization of the model.'''




########################################################################################
########################################################################################

# engine code below

########################################################################################
########################################################################################

import torch
from typing import Tuple, Optional
import copy
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc # Import for full evaluation metrics
import os
import numpy as np
from tqdm import tqdm



# --- Metrics for train_step (Added AUC-PR computation) ---
def f1_score(true_pos, false_pos, false_neg):
    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0.0
    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0.0
    if precision + recall == 0:
        return 0.0
    return 2 * (precision * recall) / (precision + recall)



def train_step(model: torch.nn.Module,
            dataloader: torch.utils.data.DataLoader, 
            loss_fn: torch.nn.Module, 
            optimizer: torch.optim.Optimizer, 
            device: torch.device,
            steps_per_epoch:int) -> Tuple[float, float, int, int, float, float, float]: 

    model.to(device)
    model.train()

    all_preds = []
    all_probs_pos_class = [] # Store probabilities for the positive class for AUC-PR
    all_labels = []
    total_loss, total_correct, total_samples = 0.0, 0.0, 0.0
    
    # Manually limit batches per epoch to match TF's steps_per_epoch
    for batch_num, (X, y) in enumerate(dataloader):
        if batch_num >= steps_per_epoch:
            break

        X, y = X.to(device), y.to(device)
        y_logits = model(X)
        y_preds = torch.argmax(y_logits, dim=1) # Predicted classes (0 or 1)
        y_probs = torch.softmax(y_logits, dim=1) # Probabilities for each class

        loss_batch = loss_fn(y_logits, y)
        batch_size_actual = y.shape[0] # Actual batch size, might be smaller at the end
        total_loss += loss_batch.item() * batch_size_actual # .item() to get scalar
        total_samples += batch_size_actual

        total_correct += (y_preds == y).sum().item()

        # Detach tensors before moving to CPU and appending to lists
        all_preds.append(y_preds.detach().cpu())
        all_labels.append(y.detach().cpu())
        all_probs_pos_class.append(y_probs[:, 1].detach().cpu()) # Store probability of positive class

        optimizer.zero_grad()
        loss_batch.backward()
        optimizer.step()

    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)
    all_probs_pos_class = torch.cat(all_probs_pos_class)

    true_pos = ((all_preds == 1) & (all_labels == 1)).sum().item()
    false_pos = ((all_preds == 1) & (all_labels == 0)).sum().item()
    false_neg = ((all_preds == 0) & (all_labels == 1)).sum().item()

    binary_accuracy = (all_preds == all_labels).sum().item() / total_samples
    f1 = f1_score(true_pos, false_pos, false_neg)

    # Calculate AUC-PR (Precision-Recall Curve AUC)
    # Ensure there are at least two unique label values for AUC
    if len(np.unique(all_labels.numpy())) > 1: # Added .numpy() for scikit-learn
        precision, recall, _ = precision_recall_curve(all_labels.numpy(), all_probs_pos_class.numpy())
        prc_auc = auc(recall, precision)
    else:
        prc_auc = 0.0 # Or np.nan, if only one class is present in the batch

    train_loss = float(total_loss / total_samples)
    train_accuracy = float(total_correct) / float(total_samples)
    
    return train_loss, train_accuracy, int(false_pos), int(false_neg), float(binary_accuracy), float(f1), float(prc_auc)




def train_model(model:torch.nn.Module,
                train_loader: torch.utils.data.DataLoader,
                optimizer: torch.optim.Optimizer,
                loss_fn: torch.nn.Module,
                device: torch.device,
                num_epochs:int,
                steps_per_epoch:int,
                patience=20,
                checkpoint_path="models/trained_model.pth"): 
    
    best_model_weights = copy.deepcopy(model.state_dict())
    best_metric = float('inf') 
    early_stopping_counter = 0
    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
    history = {
        'train_loss': [],
        'train_acc': [],
        'f1': [],
        'prc_auc': [] # Added for AUC-PR
    }

    # Create model directory if it doesn't exist
    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)

    for epoch in tqdm(range(num_epochs)):
        model.train()
    
        # Pass steps_per_epoch_tf to train_step_fn
        train_loss, train_acc, _, _, _, f1, prc_auc = train_step(model, 
                                                                 train_loader, 
                                                                 loss_fn, 
                                                                 optimizer, 
                                                                 device, 
                                                                 steps_per_epoch
                                                                )

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['f1'].append(f1)
        history['prc_auc'].append(prc_auc)

        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} - Acc: {train_acc:.4f} - F1: {f1:.4f} - PRC-AUC: {prc_auc:.4f}")

        # Learning rate scheduler step
        lr_scheduler.step(train_loss)

        # Save best model
        if (train_loss < best_metric):
            best_metric = train_loss
            best_model_weights = copy.deepcopy(model.state_dict())
            torch.save(model.state_dict(), checkpoint_path)
            print("Best model updated.")
            early_stopping_counter = 0
        else:
            early_stopping_counter += 1

        if early_stopping_counter >= patience:
            print("Early stopping triggered.")
            break

    # Restore best model
    model.load_state_dict(best_model_weights)
    return model, history
